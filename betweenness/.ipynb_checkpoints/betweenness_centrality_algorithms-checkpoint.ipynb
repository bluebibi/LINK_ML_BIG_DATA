{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Betweenness Centrality 알고리즘 작성 및 성능비교 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 필요한 모듈을 현재 Namespace에 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as net\n",
    "import operator\n",
    "from operator import itemgetter, attrgetter\n",
    "import math\n",
    "from itertools import combinations, permutations\n",
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "import codecs\n",
    "import csv\n",
    "import networkx.algorithms as algo\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "from scipy import stats\n",
    "import matplotlib\n",
    "matplotlib.use('pgf')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 4가지 DTN 데이터 셋에 대한 데이터 프로파일 구성\n",
    "- 초기에는 각 데이터 셋별로 file_name 및 num_nodes 값만 지정함\n",
    "- 이후 각 단계별로 다른 속성 데이터들을 차례로 할당하게 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "link_addition_ratio = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "data_profile = {\n",
    "    \"Infocom05\": { \n",
    "        \"file_name\": \"data_infocom05.csv\",\n",
    "        \"num_nodes\": 41,\n",
    "        \"median\": None,\n",
    "        \"mean\": None,\n",
    "        \"std\": None,\n",
    "        \"contact_weight_map\": None,\n",
    "        \n",
    "        \"graph\": {},\n",
    "        \"numberOfNodes\": {},\n",
    "        \"numberOfEdges\": {},\n",
    "        \"durationThreshold\": {},\n",
    "\n",
    "        \"density\": {}, \n",
    "        \"clustering_coefficient\": {},\n",
    "        \"diameter_cc\": {},\n",
    "\n",
    "        \"global_bet\": {},\n",
    "        \"Brandes_ego_bet\": {},\n",
    "        \"Brandes_ego_elapsed_time\": {},\n",
    "        \"Brandes_xego_bet\": {},\n",
    "        \"Brandes_xego_elapsed_time\": {},        \n",
    "\n",
    "        \"Proposed_ego_bet\": {},\n",
    "        \"Proposed_ego_elapsed_time\": {},\n",
    "        \"Proposed_xego_bet\": {},\n",
    "        \"Proposed_xego_elapsed_time\": {},      \n",
    "        \n",
    "        \"ego_global_pearson_corr\": {},\n",
    "        \"xego_global_pearson_corr\": {},\n",
    "\n",
    "        \"ego_global_spearman_corr\": {},\n",
    "        \"xego_global_spearman_corr\": {},\n",
    "        \n",
    "        \"ego_node_coverage_in_connected_component\": {},\n",
    "        \"ego_edge_coverage_in_connected_component\": {},        \n",
    "        \n",
    "        \"xego_node_coverage_in_connected_component\": {},\n",
    "        \"xego_edge_coverage_in_connected_component\": {}    \n",
    "    },\n",
    "    \"Infocom06\": {\n",
    "        \"file_name\": \"data_infocom06.csv\",\n",
    "        \"num_nodes\": 99,\n",
    "        \"median\": None,\n",
    "        \"mean\": None,\n",
    "        \"std\": None,\n",
    "        \"contact_weight_map\": None,\n",
    "\n",
    "        \"graph\": {},\n",
    "        \"numberOfNodes\": {},\n",
    "        \"numberOfEdges\": {},        \n",
    "        \"durationThreshold\": {},\n",
    "\n",
    "        \"density\": {},\n",
    "        \"clustering_coefficient\": {},\n",
    "        \"diameter_cc\": {},\n",
    "\n",
    "        \"global_bet\": {},\n",
    "        \"Brandes_ego_bet\": {},\n",
    "        \"Brandes_ego_elapsed_time\": {},\n",
    "        \"Brandes_xego_bet\": {},\n",
    "        \"Brandes_xego_elapsed_time\": {},        \n",
    "\n",
    "        \"Proposed_ego_bet\": {},\n",
    "        \"Proposed_ego_elapsed_time\": {},\n",
    "        \"Proposed_xego_bet\": {},\n",
    "        \"Proposed_xego_elapsed_time\": {},       \n",
    "        \n",
    "        \"ego_global_pearson_corr\": {},\n",
    "        \"xego_global_pearson_corr\": {},\n",
    "\n",
    "        \"ego_global_spearman_corr\": {},\n",
    "        \"xego_global_spearman_corr\": {},\n",
    "        \n",
    "        \"ego_node_coverage_in_connected_component\": {},\n",
    "        \"ego_edge_coverage_in_connected_component\": {},        \n",
    "        \n",
    "        \"xego_node_coverage_in_connected_component\": {},\n",
    "        \"xego_edge_coverage_in_connected_component\": {}  \n",
    "    },\n",
    "    \"Cambridge\": {\n",
    "        \"file_name\": \"data_cambridge.csv\",\n",
    "        \"num_nodes\": 54,\n",
    "        \"median\": None,\n",
    "        \"mean\": None,\n",
    "        \"std\": None,\n",
    "        \"contact_weight_map\": None,\n",
    "\n",
    "        \"graph\": {},\n",
    "        \"numberOfNodes\": {},\n",
    "        \"numberOfEdges\": {},        \n",
    "        \"durationThreshold\": {},\n",
    "\n",
    "        \"density\": {},\n",
    "        \"clustering_coefficient\": {},\n",
    "        \"diameter_cc\": {},\n",
    "\n",
    "        \"global_bet\": {},\n",
    "        \"Brandes_ego_bet\": {},\n",
    "        \"Brandes_ego_elapsed_time\": {},\n",
    "        \"Brandes_xego_bet\": {},\n",
    "        \"Brandes_xego_elapsed_time\": {},        \n",
    "\n",
    "        \"Proposed_ego_bet\": {},\n",
    "        \"Proposed_ego_elapsed_time\": {},\n",
    "        \"Proposed_xego_bet\": {},\n",
    "        \"Proposed_xego_elapsed_time\": {},      \n",
    "        \n",
    "        \"ego_global_pearson_corr\": {},\n",
    "        \"xego_global_pearson_corr\": {},\n",
    "\n",
    "        \"ego_global_spearman_corr\": {},\n",
    "        \"xego_global_spearman_corr\": {},\n",
    "        \n",
    "        \"ego_node_coverage_in_connected_component\": {},\n",
    "        \"ego_edge_coverage_in_connected_component\": {},        \n",
    "        \n",
    "        \"xego_node_coverage_in_connected_component\": {},\n",
    "        \"xego_edge_coverage_in_connected_component\": {}\n",
    "    }, \n",
    "    \"Intel\": {\n",
    "        \"file_name\": \"data_intel.csv\",\n",
    "        \"num_nodes\": 20,\n",
    "        \"median\": None,\n",
    "        \"mean\": None,\n",
    "        \"std\": None,\n",
    "        \"contact_weight_map\": None,\n",
    "\n",
    "        \"graph\": {},\n",
    "        \"numberOfNodes\": {},\n",
    "        \"numberOfEdges\": {},        \n",
    "        \"durationThreshold\": {},\n",
    "\n",
    "        \"density\": {},\n",
    "        \"clustering_coefficient\": {},\n",
    "        \"diameter_cc\": {},\n",
    "\n",
    "        \"global_bet\": {},\n",
    "        \"Brandes_ego_bet\": {},\n",
    "        \"Brandes_ego_elapsed_time\": {},\n",
    "        \"Brandes_xego_bet\": {},\n",
    "        \"Brandes_xego_elapsed_time\": {},        \n",
    "\n",
    "        \"Proposed_ego_bet\": {},\n",
    "        \"Proposed_ego_elapsed_time\": {},\n",
    "        \"Proposed_xego_bet\": {},\n",
    "        \"Proposed_xego_elapsed_time\": {},      \n",
    "        \n",
    "        \"ego_global_pearson_corr\": {},\n",
    "        \"xego_global_pearson_corr\": {},\n",
    "\n",
    "        \"ego_global_spearman_corr\": {},\n",
    "        \"xego_global_spearman_corr\": {},\n",
    "\n",
    "        \"ego_node_coverage_in_connected_component\": {},\n",
    "        \"ego_edge_coverage_in_connected_component\": {},        \n",
    "        \n",
    "        \"xego_node_coverage_in_connected_component\": {},\n",
    "        \"xego_edge_coverage_in_connected_component\": {}        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 각 노드들에 대한 Cumulative Contact Time 기반 Weight Map 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getContactDurationMap(dataFrame):\n",
    "    contactMap = {}\n",
    "    for i in range(0, len(dataFrame)):\n",
    "        recorder = dataFrame['Recorder'][i]\n",
    "        recordee = dataFrame['Recordee'][i]\n",
    "        contactTime = dataFrame['Contact Time'][i]\n",
    "        if (recorder, recordee) in contactMap:\n",
    "            contactMap[(recorder, recordee)] = contactMap[(recorder, recordee)] + contactTime + 1\n",
    "        else:\n",
    "            contactMap[(recorder, recordee)] = contactTime + 1\n",
    "\n",
    "    return contactMap, np.median(contactMap.values()), np.mean(contactMap.values()), np.std(contactMap.values())\n",
    "\n",
    "for data in data_profile.values():\n",
    "    df = pd.read_csv(data['file_name'])\n",
    "    data['contact_weight_map'], data['median'], data['mean'], data['std'] = getContactDurationMap(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Cambridge\n",
      "[((6, 9), 2187), ((12, 1), 4943), ((7, 12), 40849), ((1, 6), 768)]\n",
      "Mean:  9961\n",
      "Median:  2684\n",
      "Standard Deviation: 26513\n",
      "\n",
      "### Infocom06\n",
      "[((39, 70), 1327), ((43, 3), 6382), ((63, 76), 707), ((29, 44), 1366)]\n",
      "Mean:  3356\n",
      "Median:  1253\n",
      "Standard Deviation: 11578\n",
      "\n",
      "### Intel\n",
      "[((4, 7), 11018), ((1, 3), 19716), ((9, 1), 125), ((4, 8), 5037)]\n",
      "Mean: 12495\n",
      "Median:  4936\n",
      "Standard Deviation: 27158\n",
      "\n",
      "### Infocom05\n",
      "[((21, 28), 369), ((4, 36), 1117), ((7, 25), 2571), ((33, 41), 2151)]\n",
      "Mean:  3348\n",
      "Median:  1608\n",
      "Standard Deviation:  9278\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for data_name, data in data_profile.items():\n",
    "    print \"###\", data_name\n",
    "    print data['contact_weight_map'].items()[1:5]\n",
    "    print \"Mean: %5.0f\" % data['mean']\n",
    "    print \"Median: %5.0f\" % data['median']\n",
    "    print \"Standard Deviation: %5.0f\" % data['std']\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 각 데이터들에 대한 기본 통계치 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td></td><td>Infocom05</td><td>Infocom06</td><td>Cambridge</td><td>Intel</td></tr><tr><td>Number of Nodes</td><td>41</td><td>99</td><td>54</td><td>20</td></tr><tr><td>Number of contacts between nodes</td><td>22,459</td><td>170,601</td><td>4,228</td><td>1,364</td></tr><tr><td>Average number of contacts per node pair</td><td>14.3</td><td>20.2</td><td>32.0</td><td>18.9</td></tr><tr><td>Mean of accumulated contact duration per node pair (sec.)</td><td> 3348</td><td> 3356</td><td> 9961</td><td>12495</td></tr><tr><td>Median of accumulated contact duration per node pair (sec.)</td><td> 1608</td><td> 1253</td><td> 2684</td><td> 4936</td></tr><tr><td>Data collection period (sec.)</td><td>254,151 (2.9 days)</td><td>337,419 (3.9 days)</td><td>455,610 (5.3 days)</td><td>359,191 (4.2 days)</td></tr></table>"
      ],
      "text/plain": [
       "[['', 'Infocom05', 'Infocom06', 'Cambridge', 'Intel'],\n",
       " ['Number of Nodes', 41, 99, 54, 20],\n",
       " ['Number of contacts between nodes', '22,459', '170,601', '4,228', '1,364'],\n",
       " ['Average number of contacts per node pair', '14.3', '20.2', '32.0', '18.9'],\n",
       " ['Mean of accumulated contact duration per node pair (sec.)',\n",
       "  ' 3348',\n",
       "  ' 3356',\n",
       "  ' 9961',\n",
       "  '12495'],\n",
       " ['Median of accumulated contact duration per node pair (sec.)',\n",
       "  ' 1608',\n",
       "  ' 1253',\n",
       "  ' 2684',\n",
       "  ' 4936'],\n",
       " ['Data collection period (sec.)',\n",
       "  '254,151 (2.9 days)',\n",
       "  '337,419 (3.9 days)',\n",
       "  '455,610 (5.3 days)',\n",
       "  '359,191 (4.2 days)']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ListTable(list):\n",
    "    \"\"\" Overridden list class which takes a 2-dimensional list of \n",
    "        the form [[1,2,3],[4,5,6]], and renders an HTML Table in \n",
    "        IPython Notebook. \"\"\"\n",
    "    \n",
    "    def _repr_html_(self):\n",
    "        html = [\"<table>\"]\n",
    "        for row in self:\n",
    "            html.append(\"<tr>\")\n",
    "            \n",
    "            for col in row:\n",
    "                html.append(\"<td>{0}</td>\".format(col))\n",
    "            \n",
    "            html.append(\"</tr>\")\n",
    "        html.append(\"</table>\")\n",
    "        return ''.join(html)\n",
    "\n",
    "table = ListTable()\n",
    "table.append(['', 'Infocom05', 'Infocom06', 'Cambridge', 'Intel'])\n",
    "table.append(['Number of Nodes', \n",
    "              data_profile['Infocom05']['num_nodes'],\n",
    "              data_profile['Infocom06']['num_nodes'],\n",
    "              data_profile['Cambridge']['num_nodes'],\n",
    "              data_profile['Intel']['num_nodes']])\n",
    "table.append(['Number of contacts between nodes', '22,459', '170,601', '4,228', '1,364'])\n",
    "table.append(['Average number of contacts per node pair', '14.3', '20.2', '32.0', '18.9'])\n",
    "table.append(['Mean of accumulated contact duration per node pair (sec.)', \n",
    "              \"%5.0f\" % data_profile['Infocom05']['mean'],\n",
    "              \"%5.0f\" % data_profile['Infocom06']['mean'],\n",
    "              \"%5.0f\" % data_profile['Cambridge']['mean'],\n",
    "              \"%5.0f\" % data_profile['Intel']['mean']])\n",
    "table.append(['Median of accumulated contact duration per node pair (sec.)', \n",
    "              \"%5.0f\" % data_profile['Infocom05']['median'],\n",
    "              \"%5.0f\" % data_profile['Infocom06']['median'],\n",
    "              \"%5.0f\" % data_profile['Cambridge']['median'],\n",
    "              \"%5.0f\" % data_profile['Intel']['median']])\n",
    "table.append(['Data collection period (sec.)', '254,151 (2.9 days)', '337,419 (3.9 days)', '455,610 (5.3 days)', '359,191 (4.2 days)'])\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Weight Map으로 부터 각 Link Addition Ratio 별로 Duration Threshold 및 Graph 얻기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mean(values):\n",
    "    if len(values) == 0:\n",
    "        return None\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "def get_edgeList_by_threshold(n, map, median, threshold):\n",
    "    contactMap = sorted(map.iteritems(), key=operator.itemgetter(1))\n",
    "    num = int(math.ceil(float(len(contactMap)) * threshold))\n",
    "\n",
    "    edgeList = []\n",
    "    duration = 0\n",
    "    for i in range(0, num):\n",
    "        item = contactMap.pop()\n",
    "        edge = item[0]\n",
    "        duration = item[1]\n",
    "        edgeList.append(edge)\n",
    "\n",
    "    return edgeList, duration\n",
    "\n",
    "for data in data_profile.values():\n",
    "    for ratio in link_addition_ratio:\n",
    "        edgeList, duration = get_edgeList_by_threshold(data['num_nodes'], \n",
    "                                                       data['contact_weight_map'],\n",
    "                                                       data['median'], \n",
    "                                                       ratio)\n",
    "        data['durationThreshold'][ratio] = duration\n",
    "        data['graph'][ratio] = net.Graph(edgeList)\n",
    "        data['numberOfNodes'][ratio] = data['graph'][ratio].number_of_nodes()\n",
    "        data['numberOfEdges'][ratio] = data['graph'][ratio].number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Cambridge\n",
      "0.1 - 19606 - 8 - 8\n",
      "0.2 - 7786 - 12 - 15\n",
      "0.3 - 4470 - 12 - 22\n",
      "0.4 - 3246 - 12 - 29\n",
      "0.5 - 2709 - 12 - 39\n",
      "0.6 - 2187 - 12 - 48\n",
      "0.7 - 1806 - 12 - 53\n",
      "0.8 - 1566 - 12 - 59\n",
      "0.9 - 1318 - 12 - 64\n",
      "\n",
      "### Infocom06\n",
      "0.1 - 6381 - 94 - 576\n",
      "0.2 - 3883 - 97 - 1102\n",
      "0.3 - 2568 - 98 - 1605\n",
      "0.4 - 1815 - 98 - 2108\n",
      "0.5 - 1253 - 98 - 2559\n",
      "0.6 - 862 - 98 - 2995\n",
      "0.7 - 506 - 98 - 3370\n",
      "0.8 - 251 - 98 - 3716\n",
      "0.9 - 10 - 98 - 4046\n",
      "\n",
      "### Intel\n",
      "0.1 - 24329 - 5 - 5\n",
      "0.2 - 16725 - 9 - 9\n",
      "0.3 - 10915 - 9 - 13\n",
      "0.4 - 6572 - 9 - 16\n",
      "0.5 - 5037 - 9 - 19\n",
      "0.6 - 4122 - 9 - 25\n",
      "0.7 - 2211 - 9 - 28\n",
      "0.8 - 1200 - 9 - 31\n",
      "0.9 - 9 - 9 - 35\n",
      "\n",
      "### Infocom05\n",
      "0.1 - 5921 - 39 - 91\n",
      "0.2 - 3818 - 40 - 191\n",
      "0.3 - 2905 - 40 - 285\n",
      "0.4 - 2167 - 40 - 363\n",
      "0.5 - 1609 - 41 - 452\n",
      "0.6 - 1214 - 41 - 531\n",
      "0.7 - 846 - 41 - 605\n",
      "0.8 - 468 - 41 - 672\n",
      "0.9 - 141 - 41 - 740\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for data_name, data in data_profile.items():\n",
    "    print \"###\", data_name\n",
    "    for ratio in sorted(data['durationThreshold']):\n",
    "        print \"%s - %d - %d - %d\" % (ratio, \n",
    "                                     data['durationThreshold'][ratio], \n",
    "                                     data['numberOfNodes'][ratio],\n",
    "                                     data['numberOfEdges'][ratio])\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 각 Link Addition Ratio 별로 구한 그래프에 대한 density, clustering_coefficient, diameter 구하기\n",
    "- diameter는 largest connected component에 대해 계산함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for data in data_profile.values():\n",
    "    for ratio in link_addition_ratio:\n",
    "        data['density'][ratio] = net.density(data['graph'][ratio])\n",
    "        data['clustering_coefficient'][ratio] = mean(algo.clustering(data['graph'][ratio]).values())\n",
    "        data['diameter_cc'][ratio] = algo.diameter(list(algo.connected_component_subgraphs(data['graph'][ratio]))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Cambridge\n",
      "0.1 - density: 0.285714, clustering_coefficient: 0.458333, diameter_cc: 3.000000\n",
      "0.2 - density: 0.227273, clustering_coefficient: 0.347222, diameter_cc: 5.000000\n",
      "0.3 - density: 0.333333, clustering_coefficient: 0.491667, diameter_cc: 3.000000\n",
      "0.4 - density: 0.439394, clustering_coefficient: 0.648810, diameter_cc: 3.000000\n",
      "0.5 - density: 0.590909, clustering_coefficient: 0.669577, diameter_cc: 3.000000\n",
      "0.6 - density: 0.727273, clustering_coefficient: 0.781854, diameter_cc: 2.000000\n",
      "0.7 - density: 0.803030, clustering_coefficient: 0.833027, diameter_cc: 2.000000\n",
      "0.8 - density: 0.893939, clustering_coefficient: 0.893248, diameter_cc: 2.000000\n",
      "0.9 - density: 0.969697, clustering_coefficient: 0.968350, diameter_cc: 2.000000\n",
      "\n",
      "### Infocom06\n",
      "0.1 - density: 0.131778, clustering_coefficient: 0.400435, diameter_cc: 7.000000\n",
      "0.2 - density: 0.236684, clustering_coefficient: 0.483801, diameter_cc: 5.000000\n",
      "0.3 - density: 0.337681, clustering_coefficient: 0.564143, diameter_cc: 4.000000\n",
      "0.4 - density: 0.443509, clustering_coefficient: 0.635271, diameter_cc: 4.000000\n",
      "0.5 - density: 0.538397, clustering_coefficient: 0.703793, diameter_cc: 3.000000\n",
      "0.6 - density: 0.630128, clustering_coefficient: 0.781277, diameter_cc: 3.000000\n",
      "0.7 - density: 0.709026, clustering_coefficient: 0.834991, diameter_cc: 3.000000\n",
      "0.8 - density: 0.781822, clustering_coefficient: 0.879554, diameter_cc: 3.000000\n",
      "0.9 - density: 0.851252, clustering_coefficient: 0.921151, diameter_cc: 2.000000\n",
      "\n",
      "### Intel\n",
      "0.1 - density: 0.500000, clustering_coefficient: 0.466667, diameter_cc: 3.000000\n",
      "0.2 - density: 0.250000, clustering_coefficient: 0.392593, diameter_cc: 3.000000\n",
      "0.3 - density: 0.361111, clustering_coefficient: 0.411111, diameter_cc: 4.000000\n",
      "0.4 - density: 0.444444, clustering_coefficient: 0.820635, diameter_cc: 2.000000\n",
      "0.5 - density: 0.527778, clustering_coefficient: 0.736243, diameter_cc: 2.000000\n",
      "0.6 - density: 0.694444, clustering_coefficient: 0.756349, diameter_cc: 2.000000\n",
      "0.7 - density: 0.777778, clustering_coefficient: 0.765608, diameter_cc: 2.000000\n",
      "0.8 - density: 0.861111, clustering_coefficient: 0.862434, diameter_cc: 2.000000\n",
      "0.9 - density: 0.972222, clustering_coefficient: 0.972222, diameter_cc: 2.000000\n",
      "\n",
      "### Infocom05\n",
      "0.1 - density: 0.122807, clustering_coefficient: 0.367564, diameter_cc: 5.000000\n",
      "0.2 - density: 0.244872, clustering_coefficient: 0.419765, diameter_cc: 4.000000\n",
      "0.3 - density: 0.365385, clustering_coefficient: 0.480037, diameter_cc: 3.000000\n",
      "0.4 - density: 0.465385, clustering_coefficient: 0.582573, diameter_cc: 3.000000\n",
      "0.5 - density: 0.551220, clustering_coefficient: 0.663286, diameter_cc: 3.000000\n",
      "0.6 - density: 0.647561, clustering_coefficient: 0.748057, diameter_cc: 3.000000\n",
      "0.7 - density: 0.737805, clustering_coefficient: 0.827978, diameter_cc: 3.000000\n",
      "0.8 - density: 0.819512, clustering_coefficient: 0.883453, diameter_cc: 2.000000\n",
      "0.9 - density: 0.902439, clustering_coefficient: 0.946437, diameter_cc: 2.000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for data_name, data in data_profile.items():\n",
    "    print \"###\", data_name\n",
    "    for ratio in sorted(data['density']):\n",
    "        print \"%s - density: %f, clustering_coefficient: %f, diameter_cc: %f\" % (ratio, \n",
    "                                                                                 data['density'][ratio],\n",
    "                                                                                 data['clustering_coefficient'][ratio],\n",
    "                                                                                 data['diameter_cc'][ratio])\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Global Betweenness Centrality를 각 노드별로 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_Global_Betweenness_List(g):\n",
    "    return net.betweenness_centrality(g)\n",
    "\n",
    "for data in data_profile.values():\n",
    "    for ratio in link_addition_ratio:\n",
    "        data['global_bet'][ratio] = get_Global_Betweenness_List(data['graph'][ratio])       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 기존 Brandnes Betweenness 알고리즘을 통해 ego 및 xEgo Betweenness 구하기\n",
    "- [주의] 아래 코드 수행 시간: 약 2~3분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def brandes_betweenness(G):\n",
    "    betweenness=dict.fromkeys(G,0.0) # b[v]=0 for v in G\n",
    "    nodes = G\n",
    "    for s in nodes:\n",
    "        S, P, sigma = single_source_shortest_path_basic(G,s)\n",
    "        betweenness = accumulate_basic(betweenness,S,P,sigma,s)\n",
    "    betweenness = rescale(betweenness, len(G))\n",
    "    return betweenness\n",
    "\n",
    "def single_source_shortest_path_basic(G, s):\n",
    "    S=[]\n",
    "    P={}\n",
    "    for v in G:\n",
    "        P[v]=[]\n",
    "    sigma=dict.fromkeys(G,0.0)    # sigma[v]=0 for v in G\n",
    "    D={}\n",
    "    sigma[s]=1.0\n",
    "    D[s]=0\n",
    "    Q=[s]\n",
    "    while Q:   # use BFS to find shortest paths\n",
    "        v=Q.pop(0)\n",
    "        S.append(v)\n",
    "        Dv=D[v]\n",
    "        sigmav=sigma[v]\n",
    "        for w in G[v]:\n",
    "            if w not in D:\n",
    "                Q.append(w)\n",
    "                D[w]=Dv+1\n",
    "            if D[w]==Dv+1:   # this is a shortest path, count paths\n",
    "                sigma[w] += sigmav\n",
    "                P[w].append(v) # predecessors \n",
    "    return S,P,sigma\n",
    "\n",
    "def accumulate_basic(betweenness, S, P, sigma, s):\n",
    "    delta=dict.fromkeys(S, 0)\n",
    "    while S:\n",
    "        w=S.pop()\n",
    "        coeff=(1.0+delta[w])/sigma[w]\n",
    "        for v in P[w]:\n",
    "            delta[v] += sigma[v]*coeff\n",
    "        if w != s:\n",
    "            betweenness[w]+=delta[w]\n",
    "    return betweenness\n",
    "\n",
    "def rescale(betweenness, n):\n",
    "    if n <= 2:\n",
    "        scale=None  # no normalization b=0 for all nodes\n",
    "    else:\n",
    "        scale=1.0/((n-1)*(n-2))\n",
    "\n",
    "    if scale is not None:\n",
    "        for v in betweenness:\n",
    "            betweenness[v] *= scale\n",
    "    return betweenness\n",
    "\n",
    "def get_xego_graph(g, center):\n",
    "    g2 = net.Graph()\n",
    "    g2.add_node(center)\n",
    "    firstNeighbors = g.neighbors(center)\n",
    "\n",
    "    secondNeighbors = []\n",
    "    for neighbor in firstNeighbors:\n",
    "        g2.add_edge(neighbor, center)\n",
    "        secondNeighborsOfNode = g.neighbors(neighbor)\n",
    "        for nneighbor in secondNeighborsOfNode:\n",
    "            if (nneighbor == center):\n",
    "                continue\n",
    "            if nneighbor in firstNeighbors:\n",
    "                g2.add_edge(neighbor, nneighbor)\n",
    "            else:\n",
    "                g2.add_edge(neighbor, nneighbor)\n",
    "                secondNeighbors.append(nneighbor)\n",
    "    secondNeighbors = list(set(secondNeighbors))\n",
    "    return g2, firstNeighbors, secondNeighbors\n",
    "\n",
    "def get_brandnes_ego_betweenness_and_elapsed_time(g):\n",
    "    elapsed_time = 0.0\n",
    "    numNodes = g.number_of_nodes()\n",
    "    centrality_map = {} \n",
    "    for node in g.nodes():\n",
    "        egoNet = net.ego_graph(g, node)\n",
    "        #######\n",
    "        start_time = time.time()        \n",
    "        centrality_map[node] = brandes_betweenness(egoNet).get(node)\n",
    "        end_time = time.time()\n",
    "        #######\n",
    "        elapsed_time = elapsed_time + (end_time - start_time)     \n",
    "    return centrality_map, elapsed_time\n",
    "\n",
    "def get_brandnes_xego_betweenness_and_elapsed_time(g):\n",
    "    elapsed_time = 0.0\n",
    "    numNodes = g.number_of_nodes()\n",
    "    centrality_map = {}\n",
    "    for node in g.nodes():\n",
    "        xEgoNet, firstNeighbors, secondNeighbors = get_xego_graph(g, node)\n",
    "        #######\n",
    "        start_time = time.time()\n",
    "        centrality_map[node] = brandes_betweenness(xEgoNet).get(node)\n",
    "        end_time = time.time()\n",
    "        #######\n",
    "        elapsed_time = elapsed_time + (end_time - start_time)\n",
    "    return centrality_map, elapsed_time\n",
    "\n",
    "for data in data_profile.values():\n",
    "    for ratio in link_addition_ratio:\n",
    "        data['Brandes_ego_bet'][ratio], data['Brandes_ego_elapsed_time'][ratio] = get_brandnes_ego_betweenness_and_elapsed_time(data['graph'][ratio])\n",
    "        data['Brandes_xego_bet'][ratio], data['Brandes_xego_elapsed_time'][ratio] = get_brandnes_xego_betweenness_and_elapsed_time(data['graph'][ratio])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 제안하는 Ego Betweenness 및 xEgo Betweenness 알고리즘 구현\n",
    "- [주의] 아래 코드 수행 시간: 약 1~2분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def proposed_ego_betweenness(G, center, fe, node2Index, index2Node):\n",
    "    nodes = G.nodes()\n",
    "    nodes.remove(center)\n",
    "\n",
    "    lenOfFirstNeighbors = len(fe)\n",
    "    table = dict.fromkeys(index2Node.keys(), 0.0)\n",
    "    for i in index2Node.keys():\n",
    "        delta=dict.fromkeys(index2Node.keys(), 0.0)\n",
    "        table[i] = delta\n",
    "\n",
    "    betweenness = 0.0\n",
    "\n",
    "    for i in range(1, lenOfFirstNeighbors+1):\n",
    "        N_i = G[index2Node[i]].keys()\n",
    "        for j in range(i+1, lenOfFirstNeighbors+1):\n",
    "            N_j = G[index2Node[j]].keys()\n",
    "            table[i][j] = dependency1(index2Node[j], N_i, N_j)\n",
    "            betweenness += table[i][j]\n",
    "\n",
    "    betweenness = my_rescale(betweenness, len(G))\n",
    "\n",
    "    return betweenness\n",
    "\n",
    "def proposed_xego_betweenness(G, center, firstNeighbors, secondNeighbors, node2Index, index2Node):\n",
    "    nodes = G.nodes()\n",
    "    nodes.remove(center)\n",
    "\n",
    "    lenOfFirstAndSecondNeigbors = len(nodes)\n",
    "    lenOfFirstNeighbors = len(firstNeighbors)\n",
    "\n",
    "    table = dict.fromkeys(index2Node.keys(), 0.0)\n",
    "    for i in index2Node.keys():\n",
    "        delta=dict.fromkeys(index2Node.keys(), 0.0)\n",
    "        table[i] = delta\n",
    "\n",
    "    betweenness = 0.0\n",
    "\n",
    "    for i in range(1, lenOfFirstNeighbors+1):\n",
    "        N_i = G[index2Node[i]].keys()\n",
    "        for j in range(i+1, lenOfFirstNeighbors+1):\n",
    "            N_j = G[index2Node[j]].keys()\n",
    "            table[i][j] = dependency1(index2Node[j], N_i, N_j)\n",
    "            betweenness += table[i][j]\n",
    "        for j in range(lenOfFirstNeighbors+1, lenOfFirstAndSecondNeigbors+1):\n",
    "            N_j = G[index2Node[j]].keys()\n",
    "            table[i][j] = dependency2(i, N_j, node2Index, index2Node, table)\n",
    "            betweenness += table[i][j]\n",
    "\n",
    "    for i in range(lenOfFirstNeighbors+1, lenOfFirstAndSecondNeigbors+1):\n",
    "        for j in range(i+1, lenOfFirstAndSecondNeigbors+1):\n",
    "            N_j = G[index2Node[j]].keys()\n",
    "            table[i][j] = dependency2(i, N_j, node2Index, index2Node, table)\n",
    "            betweenness += table[i][j]\n",
    "\n",
    "    betweenness = my_rescale(betweenness, len(G))\n",
    "\n",
    "    return betweenness\n",
    "\n",
    "def dependency1(jnode, Ni, Nj):\n",
    "    if jnode in Ni:\n",
    "        return 0.0\n",
    "    else: \n",
    "        return 1.0 / len(set(Ni) & set(Nj))\n",
    "\n",
    "def dependency2(i, Nj, node2Index, index2Node, table):\n",
    "    values = []\n",
    "    for neighbor in Nj:\n",
    "        f = 0.0 \n",
    "        if (i < node2Index[neighbor]):\n",
    "            f = table[i][node2Index[neighbor]]\n",
    "        else: \n",
    "            f = table[node2Index[neighbor]][i]\n",
    "        if (f == 0.0): \n",
    "            return 0.0\n",
    "        else: \n",
    "            values.append(f)\n",
    "    return stats.hmean(values)\n",
    "\n",
    "def my_rescale(betweenness, n):\n",
    "    if n <= 2:\n",
    "        scale = None # no normalization b=0 for all nodes\n",
    "    else: \n",
    "        scale = 2.0/((n-1)*(n-2))\n",
    "\n",
    "    if scale is not None:\n",
    "        betweenness *= scale\n",
    "    return betweenness\n",
    "\n",
    "def get_ego_graph(g, center):\n",
    "    g2 = net.Graph()\n",
    "    node2Index = {}\n",
    "    index2Node = {}\n",
    "    g2.add_node(center)\n",
    "    firstNeighbors = g.neighbors(center)\n",
    "\n",
    "    node2Index[center] = 0\n",
    "    index2Node[0] = center\n",
    "\n",
    "    i = 1;\n",
    "    for neighbor in firstNeighbors:\n",
    "        node2Index[neighbor] = i\n",
    "        index2Node[i] = neighbor\n",
    "        i += 1;\n",
    "\n",
    "    for neighbor in firstNeighbors:\n",
    "        g2.add_edge(neighbor, center)\n",
    "        NeighborsOfNeighbors = g.neighbors(neighbor)\n",
    "        for nneighbor in NeighborsOfNeighbors:\n",
    "            if (nneighbor == center):\n",
    "                continue\n",
    "            if nneighbor in firstNeighbors:\n",
    "                g2.add_edge(neighbor, nneighbor)\n",
    "    return g2, firstNeighbors, node2Index, index2Node\n",
    "\n",
    "def get_proposed_ego_betweenness_and_elapsed_time(g):\n",
    "    elapsed_time = 0.0\n",
    "    numNodes = g.number_of_nodes()\n",
    "    centrality_map = {} \n",
    "    for node in g.nodes():\n",
    "        egoNet, firstNeighbors, node2Index, index2Node = get_ego_graph(g, node)   \n",
    "        #######\n",
    "        start_time = time.time()\n",
    "        centrality_map[node] = proposed_ego_betweenness(egoNet, node, firstNeighbors, node2Index, index2Node)\n",
    "        end_time = time.time()\n",
    "        #######\n",
    "        elapsed_time = elapsed_time + (end_time - start_time)             \n",
    "    return centrality_map, elapsed_time\n",
    "\n",
    "def get_xego_graph(g, center):\n",
    "    g2 = net.Graph()\n",
    "    node2Index = {}\n",
    "    index2Node = {}\n",
    "    g2.add_node(center)\n",
    "    firstNeighbors = g.neighbors(center)\n",
    "\n",
    "    node2Index[center] = 0\n",
    "    index2Node[0] = center\n",
    "\n",
    "    i = 1;\n",
    "    for neighbor in firstNeighbors:\n",
    "        node2Index[neighbor] = i\n",
    "        index2Node[i] = neighbor\n",
    "        i += 1;\n",
    "\n",
    "    secondNeighbors = []\n",
    "    for neighbor in firstNeighbors:\n",
    "        g2.add_edge(neighbor, center)\n",
    "        secondNeighborsOfNode = g.neighbors(neighbor)\n",
    "        for nneighbor in secondNeighborsOfNode:\n",
    "            if (nneighbor == center):\n",
    "                continue\n",
    "            if nneighbor in firstNeighbors:\n",
    "                g2.add_edge(neighbor, nneighbor)\n",
    "            else:\n",
    "                g2.add_edge(neighbor, nneighbor)\n",
    "                if not (nneighbor in secondNeighbors):\n",
    "                    secondNeighbors.append(nneighbor)\n",
    "                    node2Index[nneighbor] = i\n",
    "                    index2Node[i] = nneighbor                   \n",
    "                    i += 1\n",
    "    return g2, firstNeighbors, secondNeighbors, node2Index, index2Node\n",
    "\n",
    "def get_proposed_xego_betweenness_and_elapsed_time(g):\n",
    "    elapsed_time = 0.0\n",
    "    numNodes = g.number_of_nodes()\n",
    "    centrality_map = {} \n",
    "    for node in g.nodes():\n",
    "        xEgoNet, firstNeighbors, secondNeighbors, node2Index, index2Node = get_xego_graph(g, node)    \n",
    "        #######\n",
    "        start_time = time.time()\n",
    "        centrality_map[node] = proposed_xego_betweenness(xEgoNet, node, firstNeighbors, secondNeighbors, node2Index, index2Node)\n",
    "        end_time = time.time()\n",
    "        #######\n",
    "        elapsed_time = elapsed_time + (end_time - start_time)             \n",
    "    return centrality_map, elapsed_time\n",
    "\n",
    "for data in data_profile.values():\n",
    "    for ratio in link_addition_ratio:\n",
    "        data['Proposed_ego_bet'][ratio], data['Proposed_ego_elapsed_time'][ratio] = get_proposed_ego_betweenness_and_elapsed_time(data['graph'][ratio])\n",
    "        data['Proposed_xego_bet'][ratio], data['Proposed_xego_elapsed_time'][ratio] = get_proposed_xego_betweenness_and_elapsed_time(data['graph'][ratio])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Brandes 기법과 제안 기법으로 구한 ego 및 xego betweenness 값의 동일성 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for data_name, data in data_profile.items():\n",
    "    print \"###\", data_name\n",
    "    isEqual = True\n",
    "    for ratio in link_addition_ratio:\n",
    "        for node in data['graph'][ratio].nodes():\n",
    "            if data['Proposed_ego_bet'][ratio][node] - data['Brandes_ego_bet'][ratio][node] > 0.00001:\n",
    "                isEqual = False\n",
    "                print ratio, node, data['Brandes_ego_bet'][ratio][node], data['Proposed_ego_bet'][ratio][node]\n",
    "                break\n",
    "            if data['Proposed_xego_bet'][ratio][node] - data['Brandes_xego_bet'][ratio][node] > 0.00001:\n",
    "                isEqual = False\n",
    "                print ratio, node, data['Brandes_xego_bet'][ratio][node], data['Proposed_xego_bet'][ratio][node]\n",
    "                break    \n",
    "    if isEqual:\n",
    "        print \"All betweenness values are equals!\"\n",
    "    else:\n",
    "        print \"Fail!\"\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Correlation 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for data in data_profile.values():\n",
    "    for ratio in link_addition_ratio:\n",
    "        data['ego_global_pearson_corr'][ratio] = stats.pearsonr(data['Proposed_ego_bet'][ratio].values(), \n",
    "                                                                data['global_bet'][ratio].values())[0]\n",
    "        data['xego_global_pearson_corr'][ratio] = stats.pearsonr(data['Proposed_xego_bet'][ratio].values(), \n",
    "                                                                data['global_bet'][ratio].values())[0]\n",
    "        \n",
    "        data['ego_global_spearman_corr'][ratio] = stats.spearmanr(data['Proposed_ego_bet'][ratio].values(), \n",
    "                                                                  data['global_bet'][ratio].values())[0]\n",
    "        data['xego_global_spearman_corr'][ratio] = stats.spearmanr(data['Proposed_xego_bet'][ratio].values(), \n",
    "                                                                   data['global_bet'][ratio].values())[0]\n",
    "\n",
    "for data_name, data in data_profile.items():\n",
    "    print \"###\", data_name\n",
    "    for ratio in link_addition_ratio:\n",
    "        print \"%f: Pearson %7.5f - %7.5f, Spearman %7.5f - %7.5f\" % (ratio, data['ego_global_pearson_corr'][ratio], data['xego_global_pearson_corr'][ratio], data['ego_global_spearman_corr'][ratio], data['xego_global_spearman_corr'][ratio])\n",
    "    print     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Ego Network 및 xEgo Network 의 Coverage 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getNumOfNodesEdgesInConnectedComponent(g):\n",
    "    numNodeList = dict.fromkeys(g, 0)\n",
    "    numEdgeList = dict.fromkeys(g, 0)\n",
    "    h = list(net.connected_component_subgraphs(g))\n",
    "    numOfComponent = len(h)\n",
    "    for x in g.nodes():\n",
    "        for i in xrange(0, numOfComponent):\n",
    "            if x in h[i].nodes():\n",
    "                numNodeList[x]=len(h[i].nodes())\n",
    "                numEdgeList[x]=len(h[i].edges())\n",
    "                break\n",
    "    return numNodeList, numEdgeList\n",
    "\n",
    "def get_ego_coverage(g, numNodeInConnectedComponent, numEdgeInConnectedComponent):\n",
    "    numNodeRateList = dict.fromkeys(g, 0.0)\n",
    "    numEdgeRateList = dict.fromkeys(g, 0.0)\n",
    "    numNodes = g.number_of_nodes()\n",
    "    for node in g.nodes():\n",
    "        egoNet = get_ego_graph(g, node)[0]\n",
    "        if (numNodeInConnectedComponent[node] != 0):\n",
    "            numNodeRateList[node] = float(egoNet.number_of_nodes())/float(numNodeInConnectedComponent[node])\n",
    "        if (numEdgeInConnectedComponent[node] != 0):\n",
    "            numEdgeRateList[node] = float(egoNet.number_of_edges())/float(numEdgeInConnectedComponent[node])\t\t\n",
    "    return numNodeRateList, numEdgeRateList\n",
    "\n",
    "def get_xego_coverage(g, numNodeInConnectedComponent, numEdgeInConnectedComponent):\n",
    "    numNodeRateList = dict.fromkeys(g, 0.0)\n",
    "    numEdgeRateList = dict.fromkeys(g, 0.0)\n",
    "    numNodes = g.number_of_nodes()\n",
    "    for node in g.nodes():\n",
    "        xEgoNet = get_xego_graph(g, node)[0]\n",
    "        if (numNodeInConnectedComponent[node] != 0):\n",
    "            numNodeRateList[node] = float(xEgoNet.number_of_nodes())/float(numNodeInConnectedComponent[node])\n",
    "        if (numEdgeInConnectedComponent[node] != 0):\n",
    "            numEdgeRateList[node] = float(xEgoNet.number_of_edges())/float(numEdgeInConnectedComponent[node])\t\t\n",
    "    return numNodeRateList, numEdgeRateList\n",
    "\n",
    "def getAverageFromDict(dic):\n",
    "    sumValue = 0.0\n",
    "    for i, v in dic.items():\n",
    "        sumValue = sumValue + v\n",
    "    return sumValue / len(dic)\n",
    "\n",
    "for data in data_profile.values():\n",
    "    for ratio in link_addition_ratio:\n",
    "        numNodesCC, numEdgesCC = getNumOfNodesEdgesInConnectedComponent(data['graph'][ratio])\n",
    "        numNodes_ego_coverage, numEdges_ego_coverage = get_ego_coverage(data['graph'][ratio], numNodesCC, numEdgesCC)\n",
    "        numNodes_xego_coverage, numEdges_xego_coverage = get_xego_coverage(data['graph'][ratio], numNodesCC, numEdgesCC)\n",
    "        data['ego_node_coverage_in_connected_component'][ratio] = getAverageFromDict(numNodes_ego_coverage)\n",
    "        data['ego_edge_coverage_in_connected_component'][ratio] = getAverageFromDict(numEdges_ego_coverage)\n",
    "        data['xego_node_coverage_in_connected_component'][ratio] = getAverageFromDict(numNodes_xego_coverage)\n",
    "        data['xego_edge_coverage_in_connected_component'][ratio] = getAverageFromDict(numEdges_xego_coverage)\n",
    "        \n",
    "for data_name, data in data_profile.items():\n",
    "    print \"###\", data_name\n",
    "    for ratio in link_addition_ratio:\n",
    "        print ratio, data['ego_node_coverage_in_connected_component'][ratio], data['ego_edge_coverage_in_connected_component'][ratio],\n",
    "        print data['xego_node_coverage_in_connected_component'][ratio], data['xego_edge_coverage_in_connected_component'][ratio]\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Correlation 그래프 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SortedDisplayDict(dict):\n",
    "    def __str__(self):\n",
    "        return \"{\" + \", \".join(\"%r: %r\" % (key, self[key]) for key in sorted(self)) + \"}\"\n",
    "    def ordered_keys(self):\n",
    "        return sorted(self.keys())\n",
    "    \n",
    "dic = SortedDisplayDict(data_profile['Infocom05']['xego_global_spearman_corr'])\n",
    "\n",
    "print dic\n",
    "print [dic[x] for x in dic.ordered_keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.transforms import Bbox\n",
    "\n",
    "def full_extent(ax, pad=0.0):\n",
    "    \"\"\"Get the full extent of an axes, including axes labels, tick labels, and\n",
    "    titles.\"\"\"\n",
    "    # For text objects, we need to draw the figure first, otherwise the extents\n",
    "    # are undefined.\n",
    "    ax.figure.canvas.draw()\n",
    "    items = ax.get_xticklabels() + ax.get_yticklabels() \n",
    "    items += [ax, ax.title, ax.xaxis.label, ax.yaxis.label]\n",
    "    items += [ax, ax.title]\n",
    "    bbox = Bbox.union([item.get_window_extent() for item in items])\n",
    "\n",
    "    return bbox.expanded(1.01 + pad, 1.01 + pad)\n",
    "\n",
    "xticklabels = [r'$10\\%$', r'$20\\%$', r'$30\\%$', r'$40\\%$', r'$50\\%$', r'$60\\%$', r'$70\\%$', r'$80\\%$', r'$90\\%$']\n",
    "yticklabels = [r'$-0.6$', r'$-0.4$', r'$-0.2$', r'$0.0$', r'$0.2$', r'$0.4$', r'$0.6$', r'$0.8$', r'$1.0$', r'$1.2$']\n",
    "            \n",
    "ind = np.arange(9)\n",
    "barWidth = 0.35\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 14))\n",
    "subfigures = {}\n",
    "for data_name in data_profile.keys():\n",
    "    if data_name == 'Infocom05':\n",
    "        subfigures[data_name] = axes[0][0]\n",
    "    if data_name == 'Infocom06':\n",
    "        subfigures[data_name] = axes[0][1]\n",
    "    if data_name == 'Cambridge':\n",
    "        subfigures[data_name] = axes[1][0]\n",
    "    if data_name == 'Intel':\n",
    "        subfigures[data_name] = axes[1][1]\n",
    "\n",
    "for data_name in data_profile.keys():\n",
    "    subfigures[data_name].set_xticks(ind + barWidth/2.) \n",
    "    subfigures[data_name].set_xticklabels(xticklabels, fontsize=21)\n",
    "    subfigures[data_name].set_ylim([-0.6, 1.2])\n",
    "    subfigures[data_name].set_yticklabels(yticklabels, fontsize=21)\n",
    "    \n",
    "    corr_dic = SortedDisplayDict(data_profile[data_name]['xego_global_spearman_corr'])\n",
    "    corr_value_list = [corr_dic[x] for x in corr_dic.ordered_keys()]\n",
    "    subfigures[data_name].plot(ind + barWidth/2., \n",
    "                               corr_value_list, \n",
    "                               color='k', linestyle='-', marker='s', markersize=8, \n",
    "                               label='xego, global (spearman)')\n",
    "    \n",
    "    corr_dic = SortedDisplayDict(data_profile[data_name]['ego_global_spearman_corr'])\n",
    "    corr_value_list = [corr_dic[x] for x in corr_dic.ordered_keys()]\n",
    "    subfigures[data_name].plot(ind + barWidth/2., \n",
    "                               corr_value_list, \n",
    "                               color='k', linestyle='-', marker='|', markersize=8, \n",
    "                               label='ego, global (spearman)')\n",
    "    \n",
    "    corr_dic = SortedDisplayDict(data_profile[data_name]['xego_global_pearson_corr'])\n",
    "    corr_value_list = [corr_dic[x] for x in corr_dic.ordered_keys()]\n",
    "    subfigures[data_name].plot(ind + barWidth/2., \n",
    "                               corr_value_list, \n",
    "                               color='k', linestyle='--', marker='s', markersize=8, \n",
    "                               label='xego, global (pearson)')\n",
    "    \n",
    "    corr_dic = SortedDisplayDict(data_profile[data_name]['ego_global_pearson_corr'])\n",
    "    corr_value_list = [corr_dic[x] for x in corr_dic.ordered_keys()]\n",
    "    subfigures[data_name].plot(ind + barWidth/2., \n",
    "                               corr_value_list, \n",
    "                               color='k', linestyle='--', marker='|', markersize=8, \n",
    "                               label='ego, global (pearson)')\n",
    "    \n",
    "    subfigures[data_name].set_xlabel('Link Addition Ratio', fontsize=21)\n",
    "    subfigures[data_name].set_ylabel('Correlation Coefficient', fontsize=21)\n",
    "    subfigures[data_name].grid(True)\n",
    "    #subfigures[data_name].set_title(data_name, fontsize=21)\n",
    "    if data_name == 'Infocom05':\n",
    "        subfigures[data_name].legend(loc = 4, fontsize=18)\n",
    "        \n",
    "    extent = full_extent(subfigures[data_name]).transformed(fig.dpi_scale_trans.inverted())    \n",
    "    fig.savefig(str(data_name) + '_correlation.pdf', format='pdf', bbox_inches=extent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Clustering coefficient 및 diameter of connected component 그래프 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xticklabels = [r'$10\\%$', r'$20\\%$', r'$30\\%$', r'$40\\%$', r'$50\\%$', r'$60\\%$', r'$70\\%$', r'$80\\%$', r'$90\\%$']\n",
    "yticklabels = [r'$0.0$', r'$0.2$', r'$0.4$', r'$0.6$', r'$0.8$', r'$1.0$']\n",
    "yticklabels2 = [r'$0$', r'$2$', r'$4$', r'$6$', r'$8$', r'$10$', r'$12$']\n",
    "            \n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 14))\n",
    "subfigures = {}\n",
    "for data_name in data_profile.keys():\n",
    "    if data_name == 'Infocom05':\n",
    "        subfigures[data_name] = axes[0][0]\n",
    "    if data_name == 'Infocom06':\n",
    "        subfigures[data_name] = axes[0][1]\n",
    "    if data_name == 'Cambridge':\n",
    "        subfigures[data_name] = axes[1][0]\n",
    "    if data_name == 'Intel':\n",
    "        subfigures[data_name] = axes[1][1]\n",
    "        \n",
    "for data_name in data_profile.keys():        \n",
    "    subfigures[data_name].set_xticks(ind + barWidth/2.)\n",
    "    subfigures[data_name].set_xticklabels(xticklabels, fontsize=21)\n",
    "    subfigures[data_name].set_ylim([0.0, 1.0])\n",
    "    subfigures[data_name].set_yticklabels(yticklabels, fontsize=21)\n",
    "    \n",
    "    cluster_coef_dic = SortedDisplayDict(data_profile[data_name]['clustering_coefficient'])\n",
    "    cluster_coef_value_list = [cluster_coef_dic[x] for x in cluster_coef_dic.ordered_keys()]\n",
    "    subfigures[data_name].plot(ind + barWidth/2., cluster_coef_value_list, \n",
    "                               color='k', linestyle='-', marker='s', markersize=8, \n",
    "                               label='Clustering Coefficient')\n",
    "    \n",
    "    subfigures[data_name].set_xlabel('Link Addition Ratio', fontsize=21)\n",
    "    subfigures[data_name].set_ylabel('Clustering Coefficient', fontsize=21) \n",
    "    #subfigures[data_name].set_title(data_name, fontsize=21)\n",
    "    subfigures[data_name].grid(True)\n",
    "    if data_name == 'Infocom05':  \n",
    "        subfigures[data_name].legend(loc = 2)\n",
    "\n",
    "    subfigures[data_name] = subfigures[data_name].twinx()\n",
    "    subfigures[data_name].set_ylim([0, 12])\n",
    "    subfigures[data_name].set_yticklabels(yticklabels2, fontsize=21)\n",
    "    \n",
    "    diameter_dic = SortedDisplayDict(data_profile[data_name]['diameter_cc'])\n",
    "    diameter_list = [diameter_dic[x] for x in diameter_dic.ordered_keys()]\n",
    "    subfigures[data_name].bar(ind, diameter_list, barWidth, color='k', alpha=0.3, label='Diameter of Connected Component')\n",
    "    \n",
    "    subfigures[data_name].set_ylabel('Diameter of Connected Component', fontsize=21)\n",
    "    if data_name == 'Infocom05':  \n",
    "        subfigures[data_name].legend(loc = 4, fontsize=18)\n",
    "    subfigures[data_name].grid(True)\n",
    "    \n",
    "    fig.savefig(str(data_name) + '_clustering_and_diameter.pdf', format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xticklabels = [r'$10\\%$', r'$20\\%$', r'$30\\%$', r'$40\\%$', r'$50\\%$', r'$60\\%$', r'$70\\%$', r'$80\\%$', r'$90\\%$']\n",
    "yticklabels = [r'$0.0$', r'$0.2$', r'$0.4$', r'$0.6$', r'$0.8$', r'$1.0$']\n",
    "yticklabels2 = [r'$0$', r'$2$', r'$4$', r'$6$', r'$8$', r'$10$', r'$12$']\n",
    "            \n",
    "fig, axes = plt.subplots(1, 1, figsize=(8.719, 6.07))\n",
    "subfigures = {}\n",
    "data_name = 'Infocom05'\n",
    "subfigures[data_name] = axes\n",
    "subfigures[data_name].set_xticks(ind + barWidth/2.)\n",
    "subfigures[data_name].set_xticklabels(xticklabels, fontsize=21)\n",
    "subfigures[data_name].set_ylim([0.0, 1.0])\n",
    "subfigures[data_name].set_yticklabels(yticklabels, fontsize=21)\n",
    "\n",
    "cluster_coef_dic = SortedDisplayDict(data_profile[data_name]['clustering_coefficient'])\n",
    "cluster_coef_value_list = [cluster_coef_dic[x] for x in cluster_coef_dic.ordered_keys()]\n",
    "subfigures[data_name].plot(ind + barWidth/2., cluster_coef_value_list, \n",
    "                           color='k', linestyle='-', marker='s', markersize=8, \n",
    "                           label='Clustering Coefficient')\n",
    "\n",
    "subfigures[data_name].set_xlabel('Link Addition Ratio', fontsize=21)\n",
    "subfigures[data_name].set_ylabel('Clustering Coefficient', fontsize=21) \n",
    "#subfigures[data_name].set_title(data_name, fontsize=21)\n",
    "subfigures[data_name].grid(True)\n",
    "if data_name == 'Infocom05':  \n",
    "    subfigures[data_name].legend(loc = 2, fontsize=18)\n",
    "\n",
    "subfigures[data_name] = subfigures[data_name].twinx()\n",
    "subfigures[data_name].set_ylim([0, 12])\n",
    "subfigures[data_name].set_yticklabels(yticklabels2, fontsize=21)\n",
    "\n",
    "diameter_dic = SortedDisplayDict(data_profile[data_name]['diameter_cc'])\n",
    "diameter_list = [diameter_dic[x] for x in diameter_dic.ordered_keys()]\n",
    "subfigures[data_name].bar(ind, diameter_list, barWidth, color='k', alpha=0.3, label='Diameter of Connected Component')\n",
    "\n",
    "subfigures[data_name].set_ylabel('Diameter of Connected Component', fontsize=21)\n",
    "if data_name == 'Infocom05':  \n",
    "    subfigures[data_name].legend(loc = 4, fontsize=18)\n",
    "subfigures[data_name].grid(True)\n",
    "\n",
    "fig.savefig(str(data_name) + '_clustering_and_diameter.pdf', format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xticklabels = [r'$10\\%$', r'$20\\%$', r'$30\\%$', r'$40\\%$', r'$50\\%$', r'$60\\%$', r'$70\\%$', r'$80\\%$', r'$90\\%$']\n",
    "yticklabels = [r'$0.0$', r'$0.2$', r'$0.4$', r'$0.6$', r'$0.8$', r'$1.0$']\n",
    "yticklabels2 = [r'$0$', r'$2$', r'$4$', r'$6$', r'$8$', r'$10$', r'$12$']\n",
    "            \n",
    "fig, axes = plt.subplots(1, 1, figsize=(8.719, 6.07))\n",
    "subfigures = {}\n",
    "data_name = 'Infocom06'\n",
    "subfigures[data_name] = axes\n",
    "        \n",
    "subfigures[data_name].set_xticks(ind + barWidth/2.)\n",
    "subfigures[data_name].set_xticklabels(xticklabels, fontsize=21)\n",
    "subfigures[data_name].set_ylim([0.0, 1.0])\n",
    "subfigures[data_name].set_yticklabels(yticklabels, fontsize=21)\n",
    "\n",
    "cluster_coef_dic = SortedDisplayDict(data_profile[data_name]['clustering_coefficient'])\n",
    "cluster_coef_value_list = [cluster_coef_dic[x] for x in cluster_coef_dic.ordered_keys()]\n",
    "subfigures[data_name].plot(ind + barWidth/2., cluster_coef_value_list, \n",
    "                           color='k', linestyle='-', marker='s', markersize=8, \n",
    "                           label='Clustering Coefficient')\n",
    "\n",
    "subfigures[data_name].set_xlabel('Link Addition Ratio', fontsize=21)\n",
    "subfigures[data_name].set_ylabel('Clustering Coefficient', fontsize=21) \n",
    "#subfigures[data_name].set_title(data_name, fontsize=21)\n",
    "subfigures[data_name].grid(True)\n",
    "\n",
    "subfigures[data_name] = subfigures[data_name].twinx()\n",
    "subfigures[data_name].set_ylim([0, 12])\n",
    "subfigures[data_name].set_yticklabels(yticklabels2, fontsize=21)\n",
    "\n",
    "diameter_dic = SortedDisplayDict(data_profile[data_name]['diameter_cc'])\n",
    "diameter_list = [diameter_dic[x] for x in diameter_dic.ordered_keys()]\n",
    "subfigures[data_name].bar(ind, diameter_list, barWidth, color='k', alpha=0.3, label='Diameter of Connected Component')\n",
    "\n",
    "subfigures[data_name].set_ylabel('Diameter of Connected Component', fontsize=21)\n",
    "subfigures[data_name].grid(True)\n",
    "\n",
    "fig.savefig(str(data_name) + '_clustering_and_diameter.pdf', format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xticklabels = [r'$10\\%$', r'$20\\%$', r'$30\\%$', r'$40\\%$', r'$50\\%$', r'$60\\%$', r'$70\\%$', r'$80\\%$', r'$90\\%$']\n",
    "yticklabels = [r'$0.0$', r'$0.2$', r'$0.4$', r'$0.6$', r'$0.8$', r'$1.0$']\n",
    "yticklabels2 = [r'$0$', r'$2$', r'$4$', r'$6$', r'$8$', r'$10$', r'$12$']\n",
    "            \n",
    "fig, axes = plt.subplots(1, 1, figsize=(8.719, 6.07))\n",
    "subfigures = {}\n",
    "data_name = 'Cambridge'\n",
    "subfigures[data_name] = axes\n",
    "        \n",
    "subfigures[data_name].set_xticks(ind + barWidth/2.)\n",
    "subfigures[data_name].set_xticklabels(xticklabels, fontsize=21)\n",
    "subfigures[data_name].set_ylim([0.0, 1.0])\n",
    "subfigures[data_name].set_yticklabels(yticklabels, fontsize=21)\n",
    "\n",
    "cluster_coef_dic = SortedDisplayDict(data_profile[data_name]['clustering_coefficient'])\n",
    "cluster_coef_value_list = [cluster_coef_dic[x] for x in cluster_coef_dic.ordered_keys()]\n",
    "subfigures[data_name].plot(ind + barWidth/2., cluster_coef_value_list, \n",
    "                           color='k', linestyle='-', marker='s', markersize=8, \n",
    "                           label='Clustering Coefficient')\n",
    "\n",
    "subfigures[data_name].set_xlabel('Link Addition Ratio', fontsize=21)\n",
    "subfigures[data_name].set_ylabel('Clustering Coefficient', fontsize=21) \n",
    "#subfigures[data_name].set_title(data_name, fontsize=21)\n",
    "subfigures[data_name].grid(True)\n",
    "\n",
    "subfigures[data_name] = subfigures[data_name].twinx()\n",
    "subfigures[data_name].set_ylim([0, 12])\n",
    "subfigures[data_name].set_yticklabels(yticklabels2, fontsize=21)\n",
    "\n",
    "diameter_dic = SortedDisplayDict(data_profile[data_name]['diameter_cc'])\n",
    "diameter_list = [diameter_dic[x] for x in diameter_dic.ordered_keys()]\n",
    "subfigures[data_name].bar(ind, diameter_list, barWidth, color='k', alpha=0.3, label='Diameter of Connected Component')\n",
    "\n",
    "subfigures[data_name].set_ylabel('Diameter of Connected Component', fontsize=21)\n",
    "subfigures[data_name].grid(True)\n",
    "\n",
    "fig.savefig(str(data_name) + '_clustering_and_diameter.pdf', format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xticklabels = [r'$10\\%$', r'$20\\%$', r'$30\\%$', r'$40\\%$', r'$50\\%$', r'$60\\%$', r'$70\\%$', r'$80\\%$', r'$90\\%$']\n",
    "yticklabels = [r'$0.0$', r'$0.2$', r'$0.4$', r'$0.6$', r'$0.8$', r'$1.0$']\n",
    "yticklabels2 = [r'$0$', r'$2$', r'$4$', r'$6$', r'$8$', r'$10$', r'$12$']\n",
    "            \n",
    "fig, axes = plt.subplots(1, 1, figsize=(8.719, 6.07))\n",
    "subfigures = {}\n",
    "data_name = 'Intel'\n",
    "subfigures[data_name] = axes\n",
    "        \n",
    "subfigures[data_name].set_xticks(ind + barWidth/2.)\n",
    "subfigures[data_name].set_xticklabels(xticklabels, fontsize=21)\n",
    "subfigures[data_name].set_ylim([0.0, 1.0])\n",
    "subfigures[data_name].set_yticklabels(yticklabels, fontsize=21)\n",
    "\n",
    "cluster_coef_dic = SortedDisplayDict(data_profile[data_name]['clustering_coefficient'])\n",
    "cluster_coef_value_list = [cluster_coef_dic[x] for x in cluster_coef_dic.ordered_keys()]\n",
    "subfigures[data_name].plot(ind + barWidth/2., cluster_coef_value_list, \n",
    "                           color='k', linestyle='-', marker='s', markersize=8, \n",
    "                           label='Clustering Coefficient')\n",
    "\n",
    "subfigures[data_name].set_xlabel('Link Addition Ratio', fontsize=21)\n",
    "subfigures[data_name].set_ylabel('Clustering Coefficient', fontsize=21) \n",
    "#subfigures[data_name].set_title(data_name, fontsize=21)\n",
    "subfigures[data_name].grid(True)\n",
    "\n",
    "subfigures[data_name] = subfigures[data_name].twinx()\n",
    "subfigures[data_name].set_ylim([0, 12])\n",
    "subfigures[data_name].set_yticklabels(yticklabels2, fontsize=21)\n",
    "\n",
    "diameter_dic = SortedDisplayDict(data_profile[data_name]['diameter_cc'])\n",
    "diameter_list = [diameter_dic[x] for x in diameter_dic.ordered_keys()]\n",
    "subfigures[data_name].bar(ind, diameter_list, barWidth, color='k', alpha=0.3, label='Diameter of Connected Component')\n",
    "\n",
    "subfigures[data_name].set_ylabel('Diameter of Connected Component', fontsize=21)\n",
    "subfigures[data_name].grid(True)\n",
    "\n",
    "fig.savefig(str(data_name) + '_clustering_and_diameter.pdf', format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Coverage 그래프 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xticklabels = [r'$10\\%$', r'$20\\%$', r'$30\\%$', r'$40\\%$', r'$50\\%$', r'$60\\%$', r'$70\\%$', r'$80\\%$', r'$90\\%$']\n",
    "yticklabels = [r'$0.0$', r'$0.2$', r'$0.4$', r'$0.6$', r'$0.8$', r'$1.0$', r'$1.2$']\n",
    "            \n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 14))\n",
    "subfigures = {}\n",
    "for data_name in data_profile.keys():\n",
    "    if data_name == 'Infocom05':\n",
    "        subfigures[data_name] = axes[0][0]\n",
    "    if data_name == 'Infocom06':\n",
    "        subfigures[data_name] = axes[0][1]\n",
    "    if data_name == 'Cambridge':\n",
    "        subfigures[data_name] = axes[1][0]\n",
    "    if data_name == 'Intel':\n",
    "        subfigures[data_name] = axes[1][1]\n",
    "\n",
    "for data_name in data_profile.keys():\n",
    "    subfigures[data_name].set_xticks(ind + barWidth/2.)\n",
    "    subfigures[data_name].set_xticklabels(xticklabels, fontsize=21)\n",
    "    subfigures[data_name].set_ylim([0.0, 1.2])\n",
    "    subfigures[data_name].set_yticklabels(yticklabels, fontsize=21)\n",
    "    \n",
    "    coverage_dic = SortedDisplayDict(data_profile[data_name]['xego_node_coverage_in_connected_component'])\n",
    "    coverage_list = [coverage_dic[x] for x in coverage_dic.ordered_keys()]\n",
    "    subfigures[data_name].plot(ind+barWidth/2., coverage_list, \n",
    "                               color='k', linestyle='-', marker='s', markersize=8, \n",
    "                               label=r'Mean vertex coverage of x-ego networks') \n",
    "\n",
    "    coverage_dic = SortedDisplayDict(data_profile[data_name]['xego_edge_coverage_in_connected_component'])\n",
    "    coverage_list = [coverage_dic[x] for x in coverage_dic.ordered_keys()]    \n",
    "    subfigures[data_name].plot(ind+barWidth/2., coverage_list, \n",
    "                               color='k', linestyle='-', marker='|', markersize=8, \n",
    "                               label=r'Mean edge coverage of x-ego networks')\n",
    "\n",
    "    coverage_dic = SortedDisplayDict(data_profile[data_name]['ego_node_coverage_in_connected_component'])\n",
    "    coverage_list = [coverage_dic[x] for x in coverage_dic.ordered_keys()]        \n",
    "    subfigures[data_name].plot(ind+barWidth/2., coverage_list, \n",
    "                               color='k', linestyle='--', marker='s', markersize=8, \n",
    "                               label=r'Mean vertex coverage of ego networks') \n",
    "\n",
    "    coverage_dic = SortedDisplayDict(data_profile[data_name]['ego_edge_coverage_in_connected_component'])\n",
    "    coverage_list = [coverage_dic[x] for x in coverage_dic.ordered_keys()]        \n",
    "    subfigures[data_name].plot(ind+barWidth/2., coverage_list, \n",
    "                               color='k', linestyle='--', marker='|', markersize=8, \n",
    "                               label=r'Mean edge coverage of ego networks')\n",
    "    \n",
    "    subfigures[data_name].set_xlabel('Link Addition Ratio', fontsize=21)\n",
    "    subfigures[data_name].set_ylabel('Coverage', fontsize=21)\n",
    "    #subfigures[data_name].set_title(data_name, fontsize=21)\n",
    "    if data_name == 'Infocom05':  \n",
    "        subfigures[data_name].legend(loc = 4, fontsize=18)\n",
    "    subfigures[data_name].grid(True)\n",
    "    \n",
    "    extent = full_extent(subfigures[data_name]).transformed(fig.dpi_scale_trans.inverted()) \n",
    "    fig.savefig(str(data_name) + '_graph_info_2.pdf', format='pdf', bbox_inches=extent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Algorithm execution speed 그래프 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "width = 0.35\n",
    "xticklabels = [r'$10\\%$', r'$20\\%$', r'$30\\%$', r'$40\\%$', r'$50\\%$', r'$60\\%$', r'$70\\%$', r'$80\\%$', r'$90\\%$']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 14))\n",
    "subfigures = {}\n",
    "for data_name in data_profile.keys():\n",
    "    if data_name == 'Infocom05':\n",
    "        subfigures[data_name] = axes[0][0]\n",
    "    if data_name == 'Infocom06':\n",
    "        subfigures[data_name] = axes[0][1]\n",
    "    if data_name == 'Cambridge':\n",
    "        subfigures[data_name] = axes[1][0]\n",
    "    if data_name == 'Intel':\n",
    "        subfigures[data_name] = axes[1][1]\n",
    "\n",
    "for data_name in data_profile.keys():\n",
    "    subfigures[data_name].set_xticks(ind + width/2.)\n",
    "    subfigures[data_name].set_xticklabels(xticklabels, fontsize=21)\n",
    "    subfigures[data_name].tick_params(labelsize=21)\n",
    "\n",
    "    time_dic = SortedDisplayDict(data_profile[data_name]['Brandes_xego_elapsed_time'])\n",
    "    time_list = [time_dic[x] for x in time_dic.ordered_keys()]    \n",
    "    subfigures[data_name].plot(ind + width/2., time_list, \n",
    "                               color='k', linestyle='--', marker='o', markersize=7, label='Brandes')\n",
    "\n",
    "    time_dic = SortedDisplayDict(data_profile[data_name]['Proposed_xego_elapsed_time'])\n",
    "    time_list = [time_dic[x] for x in time_dic.ordered_keys()]\n",
    "    subfigures[data_name].plot(ind + width/2., time_list, \n",
    "                               color='k', linestyle='-', marker='x', markersize=7, label='Proposed')    \n",
    "\n",
    "    subfigures[data_name].set_xlabel('Link Addition Ratio', fontsize=21)\n",
    "    subfigures[data_name].set_ylabel('Mean Execution Time (sec.)', fontsize=21)\n",
    "    #subfigures[data_name].set_title(data_name, fontsize=21)\n",
    "    if data_name == 'Infocom05':  \n",
    "        subfigures[data_name].legend(loc = 0, fontsize=18)\n",
    "    subfigures[data_name].grid(True)\n",
    "    \n",
    "    extent = full_extent(subfigures[data_name]).transformed(fig.dpi_scale_trans.inverted()) \n",
    "    fig.savefig(str(data_name) + '_xEgo_elapsed_time.pdf', bbox_inches=extent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. 제안 Algorithm의 높은 효율을 입증하는 Skip ratio 그래프 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 알고리즘 내부에서 수행 횟수에 대한 통계치는 외부 프로그램에 의해 산출함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "width = 0.35\n",
    "xticklabels = [r'$10\\%$', r'$20\\%$', r'$30\\%$', r'$40\\%$', r'$50\\%$', r'$60\\%$', r'$70\\%$', r'$80\\%$', r'$90\\%$']\n",
    "yticklabels = [r'$0\\%$', r'$20\\%$', r'$40\\%$', r'$60\\%$', r'$80\\%$', r'$100\\%$']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 14))\n",
    "subfigures = {}\n",
    "for data_name in data_profile.keys():\n",
    "    if data_name == 'Infocom05':\n",
    "        subfigures[data_name] = axes[0][0]\n",
    "    if data_name == 'Infocom06':\n",
    "        subfigures[data_name] = axes[0][1]\n",
    "    if data_name == 'Cambridge':\n",
    "        subfigures[data_name] = axes[1][0]\n",
    "    if data_name == 'Intel':\n",
    "        subfigures[data_name] = axes[1][1]\n",
    "\n",
    "for data_name in data_profile.keys():\n",
    "    if data_name == 'Infocom05':\n",
    "        data_type = 0\n",
    "    if data_name == 'Infocom06':\n",
    "        data_type = 1\n",
    "    if data_name == 'Cambridge':\n",
    "        data_type = 2\n",
    "    if data_name == 'Intel':\n",
    "        data_type = 3\n",
    "        \n",
    "    subfigures[data_name].set_xticks(ind + width/2.)\n",
    "    subfigures[data_name].set_xticklabels(xticklabels, fontsize=21)\n",
    "    subfigures[data_name].tick_params(labelsize=21)\n",
    "    subfigures[data_name].set_ylim([0.0, 1.1])   \n",
    "    subfigures[data_name].set_yticklabels(yticklabels, fontsize=21)\n",
    "    subfigures[data_name].plot(ind + width/2., xEgo_execution_dependency2[data_type],\n",
    "                               color='k', linestyle='-', marker='x', markersize=7, label='dependency2')\n",
    "    subfigures[data_name].plot(ind + width/2., xEgo_execution_dependency1[data_type], \n",
    "                               color='k', linestyle='--', marker='o', markersize=7, label='dependency1')\n",
    "    subfigures[data_name].set_xlabel('Link Addition Ratio', fontsize=21)\n",
    "    subfigures[data_name].set_ylabel('Skip Ratio', fontsize=21)\n",
    "    #subfigures[data_name].set_title(data_name, fontsize=21)\n",
    "    if data_name == 'Infocom05':  \n",
    "        subfigures[data_name].legend(loc = 4, fontsize=18)\n",
    "    subfigures[data_name].grid(True)\n",
    "    \n",
    "    extent = full_extent(subfigures[data_name]).transformed(fig.dpi_scale_trans.inverted()) \n",
    "    fig.savefig(str(data_name) + '_xEgo_skip_rate.pdf', bbox_inches=extent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. 시간이 경과함에 따라 Betweenness 사이의 Correlation 변화 그래프 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myfile = [None, None, None]\n",
    "correlationTickList = [None, None, None]\n",
    "correlationXEgoGlobal = [None, None, None]\n",
    "correlationEgoGlobal = [None, None, None]\n",
    "\n",
    "myfile[0] = open('1_cor_spearman_time-3600-1-undirectional.csv', 'r')\n",
    "myfile[1] = open('1_cor_spearman_time-43200-1-undirectional.csv', 'r')\n",
    "myfile[2] = open('1_cor_spearman_time-86400-1-undirectional.csv', 'r')\n",
    "\n",
    "for index in range(0,3):\n",
    "    r = csv.reader(myfile[index])\n",
    "    i = 0\n",
    "    for row in r:\n",
    "        if i == 0:\n",
    "            frow = [int(num) for num in row]\n",
    "            correlationTickList[index] = frow           \n",
    "        if i == 1:\n",
    "            frow = [float(num) for num in row]          \n",
    "            correlationXEgoGlobal[index] = frow\n",
    "        if i == 2:\n",
    "            frow = [float(num) for num in row]                      \n",
    "            correlationEgoGlobal[index] = frow\n",
    "        i += 1;\n",
    "\n",
    "    myfile[index].close()\n",
    "\n",
    "allzero = []\n",
    "for i in range(0, len(correlationTickList[0]) - len(correlationTickList[1])):\n",
    "    allzero.append(0.0)\n",
    "correlationXEgoGlobal[1] = allzero + correlationXEgoGlobal[1] \n",
    "correlationEgoGlobal[1] = allzero + correlationEgoGlobal[1]\n",
    "\n",
    "allzero = []\n",
    "for i in range(0, len(correlationTickList[0]) - len(correlationTickList[2])):\n",
    "    allzero.append(0.0)\n",
    "correlationXEgoGlobal[2] = allzero + correlationXEgoGlobal[2] \n",
    "correlationEgoGlobal[2] = allzero + correlationEgoGlobal[2]\n",
    "\n",
    "min_val = 1.0\n",
    "ratio = 0.0 \n",
    "for i in correlationXEgoGlobal[0]:\n",
    "    if (i != 0.0 and i < min_val):\n",
    "        min_val = i\n",
    "    if (i >= 0.8): \n",
    "        ratio = ratio + 1\n",
    "\n",
    "min_val = 1.0\n",
    "ratio = 0.0\n",
    "for i in correlationXEgoGlobal[1]:\n",
    "    if (i != 0.0 and i < min_val):\n",
    "        min_val = i\n",
    "    if (i >= 0.8): \n",
    "        ratio = ratio + 1\n",
    "\n",
    "min_val = 1.0\n",
    "ratio = 0.0\n",
    "for i in correlationXEgoGlobal[2]:\n",
    "    if (i != 0.0 and i < min_val):\n",
    "        min_val = i\n",
    "    if (i >= 0.8): \n",
    "        ratio = ratio + 1\n",
    "\n",
    "font = {'family': 'serif', 'color': 'black', 'weight': 'normal', 'size': 16}\n",
    "\n",
    "ind = np.arange(len(correlationTickList[0]))\n",
    "barWidth = 0.5\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex = True, figsize=(20, 14))\n",
    "\n",
    "xind = [ind[0]+barWidth/2., \n",
    "        ind[int((len(correlationTickList[0])-1)/4)]+barWidth/2., \n",
    "        ind[int((len(correlationTickList[0])-1)/2)]+barWidth/2., \n",
    "        ind[int((len(correlationTickList[0])-1)*3/4)]+barWidth/2., \n",
    "        ind[len(correlationTickList[0])-1]+barWidth/2.] \n",
    "\n",
    "#step: 1\n",
    "xtickLable = [correlationTickList[0][0]-20732, \n",
    "              correlationTickList[0][int((len(correlationTickList[0])-1)/4)]-20732, \n",
    "              correlationTickList[0][int((len(correlationTickList[0])-1)/2)]-20732, \n",
    "              correlationTickList[0][int((len(correlationTickList[0])-1)*3/4)]-20732, \n",
    "              correlationTickList[0][len(correlationTickList[0])-1]-20732]\n",
    "ax1.tick_params(labelsize=14)\n",
    "ax1.set_ylim(ymin=0.0, ymax=1.2)\n",
    "\n",
    "#tick start - 107132\n",
    "correlationXEgoGlobal2 = np.array(correlationXEgoGlobal[2])\n",
    "correlationEgoGlobal2 = np.array(correlationEgoGlobal[2])\n",
    "ax1.plot(ind+barWidth/2., correlationXEgoGlobal2, lw=1, ls=\"-\", c=\"black\")\n",
    "ax1.plot(ind+barWidth/2., correlationEgoGlobal2, lw=1, ls=\":\", c=\"black\")\n",
    "\n",
    "ax1.fill_between(ind+barWidth/2., \n",
    "                 correlationEgoGlobal2, correlationXEgoGlobal2, \n",
    "                 where=correlationXEgoGlobal2 >= correlationEgoGlobal2, \n",
    "                 facecolor='green', alpha=0.2)\n",
    "\n",
    "ax1.fill_between(ind+barWidth/2., \n",
    "                 correlationXEgoGlobal2, correlationEgoGlobal2, \n",
    "                 where=correlationXEgoGlobal2 < correlationEgoGlobal2, \n",
    "                 facecolor='red', alpha=0.2)\n",
    "\n",
    "ax1.text(2000, 0.1, r'$\\omega=86400,\\,\\delta=1$', fontdict=font, fontsize=18)\n",
    "#ax1.annotate('107132', xy=(107132, 0.0),  xycoords='data', xytext=(127132, 0.2), textcoords='offset points', arrowprops=dict(facecolor='black', shrink=0.05), horizontalalignment='right', verticalalignment='top')\n",
    "\n",
    "#tick start - 63932\n",
    "ax2.set_ylim(ymin=0.0, ymax=1.2)\n",
    "ax2.tick_params(labelsize=14)\n",
    "correlationXEgoGlobal2 = np.array(correlationXEgoGlobal[1])\n",
    "correlationEgoGlobal2 = np.array(correlationEgoGlobal[1])\n",
    "ax2.plot(ind+barWidth/2., correlationXEgoGlobal2, lw=1, ls=\"-\", c=\"black\")\n",
    "ax2.plot(ind+barWidth/2., correlationEgoGlobal2, lw=1, ls=\":\", c=\"black\")\n",
    "\n",
    "ax2.fill_between(ind+barWidth/2., \n",
    "                 correlationEgoGlobal2, correlationXEgoGlobal2, \n",
    "                 where=correlationXEgoGlobal2 >= correlationEgoGlobal2, \n",
    "                 facecolor='green', alpha=0.2)\n",
    "\n",
    "ax2.fill_between(ind+barWidth/2., correlationXEgoGlobal2, \n",
    "                 correlationEgoGlobal2, where=correlationXEgoGlobal2 < correlationEgoGlobal2, \n",
    "                 facecolor='red', alpha=0.2)\n",
    "\n",
    "ax2.text(2000, 0.1, r'$\\omega=43200,\\,\\delta=1$', fontdict=font, fontsize=18)\n",
    "#ax2.annotate('63932', xy=(63932, 0.0),  xycoords='data', xytext=(63932, 0.2), textcoords='offset points', arrowprops=dict(facecolor='black', shrink=0.05), horizontalalignment='right', verticalalignment='top')\n",
    "\n",
    "ax3.set_ylim(ymin=0.0, ymax=1.2)\n",
    "ax3.set_ylabel('Spearman Correlation', position=(0.1,1.7), fontsize=14)\n",
    "ax3.set_xlabel('Time Slot', fontsize=14)\n",
    "ax3.set_xticks(xind)\n",
    "ax3.tick_params(labelsize=14)\n",
    "ax3.set_xticklabels(xtickLable, fontsize=14)\n",
    "correlationXEgoGlobal2 = np.array(correlationXEgoGlobal[0])\n",
    "correlationEgoGlobal2 = np.array(correlationEgoGlobal[0])\n",
    "ax3.plot(ind+barWidth/2., correlationXEgoGlobal2, lw=1, ls=\"-\", c=\"black\")\n",
    "ax3.plot(ind+barWidth/2., correlationEgoGlobal2, lw=1, ls=\":\", c=\"black\")\n",
    "\n",
    "ax3.fill_between(ind+barWidth/2., \n",
    "                 correlationEgoGlobal2, correlationXEgoGlobal2, \n",
    "                 where=correlationXEgoGlobal2 >= correlationEgoGlobal2, \n",
    "                 facecolor='green', alpha=0.2)\n",
    "\n",
    "ax3.fill_between(ind+barWidth/2., \n",
    "                 correlationXEgoGlobal2, correlationEgoGlobal2, \n",
    "                 where=correlationXEgoGlobal2 < correlationEgoGlobal2, \n",
    "                 facecolor='red', alpha=0.2)\n",
    "ax3.text(2000, 0.1, r'$\\omega=3600,\\,\\delta=1$', fontdict=font, fontsize=18)\n",
    "\n",
    "#plt.savefig('cor_spearman_time-step-1.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myfile = [None, None, None]\n",
    "correlationTickList = [None, None, None]\n",
    "correlationXEgoGlobal = [None, None, None]\n",
    "correlationEgoGlobal = [None, None, None]\n",
    "\n",
    "myfile[0] = open('1_cor_spearman_time-3600-3600-undirectional.csv', 'r')\n",
    "myfile[1] = open('1_cor_spearman_time-43200-3600-undirectional.csv', 'r')\n",
    "myfile[2] = open('1_cor_spearman_time-86400-3600-undirectional.csv', 'r')\n",
    "\n",
    "for index in range(0,3):\n",
    "    r = csv.reader(myfile[index])\n",
    "    i = 0\n",
    "\n",
    "    for row in r:\n",
    "        if i == 0:\n",
    "            frow = [int(num) for num in row]\n",
    "            correlationTickList[index] = frow           \n",
    "        if i == 1:\n",
    "            frow = [float(num) for num in row]          \n",
    "            correlationXEgoGlobal[index] = frow\n",
    "        if i == 2:\n",
    "            frow = [float(num) for num in row]                      \n",
    "            correlationEgoGlobal[index] = frow\n",
    "        i += 1;\n",
    "    myfile[index].close()\n",
    "\n",
    "allzero = []\n",
    "for i in range(0, len(correlationTickList[0]) - len(correlationTickList[1])):\n",
    "    allzero.append(0.0)\n",
    "correlationXEgoGlobal[1] = allzero + correlationXEgoGlobal[1] \n",
    "correlationEgoGlobal[1] = allzero + correlationEgoGlobal[1]\n",
    "\n",
    "allzero = []\n",
    "for i in range(0, len(correlationTickList[0]) - len(correlationTickList[2])):\n",
    "    allzero.append(0.0)\n",
    "correlationXEgoGlobal[2] = allzero + correlationXEgoGlobal[2] \n",
    "correlationEgoGlobal[2] = allzero + correlationEgoGlobal[2]\n",
    "\n",
    "min_val = 1.0\n",
    "ratio = 0.0 \n",
    "for i in correlationXEgoGlobal[0]:\n",
    "    if (i != 0.0 and i < min_val):\n",
    "        min_val = i\n",
    "    if (i >= 0.8): \n",
    "        ratio = ratio + 1\n",
    "\n",
    "min_val = 1.0\n",
    "ratio = 0.0\n",
    "for i in correlationXEgoGlobal[1]:\n",
    "    if (i != 0.0 and i < min_val):\n",
    "        min_val = i\n",
    "    if (i >= 0.8): \n",
    "        ratio = ratio + 1\n",
    "\n",
    "min_val = 1.0\n",
    "ratio = 0.0\n",
    "for i in correlationXEgoGlobal[2]:\n",
    "    if (i != 0.0 and i < min_val):\n",
    "        min_val = i\n",
    "    if (i >= 0.8): \n",
    "        ratio = ratio + 1\n",
    "\n",
    "font = {'family': 'serif', 'color': 'black', 'weight': 'normal', 'size': 16}\n",
    "\n",
    "ind = np.arange(len(correlationTickList[0]))\n",
    "barWidth=0.5\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex = True, figsize=(20, 14))\n",
    "\n",
    "xind = [ind[0]+barWidth/2., \n",
    "        ind[int((len(correlationTickList[0])-1)/4)]+barWidth/2., \n",
    "        ind[int((len(correlationTickList[0])-1)/2)]+barWidth/2., \n",
    "        ind[int((len(correlationTickList[0])-1)*3/4)]+barWidth/2., \n",
    "        ind[len(correlationTickList[0])-1]+barWidth/2.]    \n",
    "\n",
    "#step: 3600\n",
    "xtickLable = [3600, 66237, 128875, 191513, correlationTickList[0][len(correlationTickList[0])-1]-24331]\n",
    "\n",
    "ax1.set_ylim(ymin=0.0, ymax=1.2)\n",
    "ax1.tick_params(labelsize=14)\n",
    "\n",
    "#tick start - 107132\n",
    "correlationXEgoGlobal2 = np.array(correlationXEgoGlobal[2])\n",
    "correlationEgoGlobal2 = np.array(correlationEgoGlobal[2])\n",
    "ax1.plot(ind+barWidth/2., correlationXEgoGlobal2, lw=1, ls=\"-\", c=\"black\")\n",
    "ax1.plot(ind+barWidth/2., correlationEgoGlobal2, lw=1, ls=\":\", c=\"black\")\n",
    "\n",
    "ax1.fill_between(ind+barWidth/2., \n",
    "                 correlationEgoGlobal2, correlationXEgoGlobal2, \n",
    "                 where=correlationXEgoGlobal2 >= correlationEgoGlobal2, \n",
    "                 facecolor='green', alpha=0.2)\n",
    "\n",
    "ax1.fill_between(ind+barWidth/2., \n",
    "                 correlationXEgoGlobal2, correlationEgoGlobal2, \n",
    "                 where=correlationXEgoGlobal2 < correlationEgoGlobal2, \n",
    "                 facecolor='red', alpha=0.2)    \n",
    "ax1.text(1, 0.1, r'$\\omega=86400,\\,\\delta=3600$', fontdict=font, fontsize=18)\n",
    "\n",
    "#tick start - 63932\n",
    "ax2.set_ylim(ymin=0.0, ymax=1.2)\n",
    "ax2.tick_params(labelsize=14)\n",
    "correlationXEgoGlobal2 = np.array(correlationXEgoGlobal[1])\n",
    "correlationEgoGlobal2 = np.array(correlationEgoGlobal[1])\n",
    "ax2.plot(ind+barWidth/2., correlationXEgoGlobal2, lw=1, ls=\"-\", c=\"black\")\n",
    "ax2.plot(ind+barWidth/2., correlationEgoGlobal2, lw=1, ls=\":\", c=\"black\")\n",
    "\n",
    "ax2.fill_between(ind+barWidth/2., \n",
    "                 correlationEgoGlobal2, correlationXEgoGlobal2, \n",
    "                 where=correlationXEgoGlobal2 >= correlationEgoGlobal2, \n",
    "                 facecolor='green', alpha=0.2)\n",
    "\n",
    "ax2.fill_between(ind+barWidth/2., \n",
    "                 correlationXEgoGlobal2, correlationEgoGlobal2, \n",
    "                 where=correlationXEgoGlobal2 < correlationEgoGlobal2, \n",
    "                 facecolor='red', alpha=0.2)    \n",
    "\n",
    "ax2.text(1, 0.1, r'$\\omega=43200,\\,\\delta=3600$', fontdict=font, fontsize=18)\n",
    "\n",
    "ax3.set_ylim(ymin=0.0, ymax=1.2)\n",
    "ax3.set_ylabel('Spearman Correlation', position=(0.1,1.7), fontsize=14)\n",
    "ax3.set_xlabel('Time Slot', fontsize=14)\n",
    "ax3.set_xticks(xind)\n",
    "ax3.tick_params(labelsize=14)\n",
    "ax3.set_xticklabels(xtickLable, fontsize=14)\n",
    "correlationXEgoGlobal2 = np.array(correlationXEgoGlobal[0])\n",
    "correlationEgoGlobal2 = np.array(correlationEgoGlobal[0])\n",
    "ax3.plot(ind+barWidth/2., correlationXEgoGlobal2, lw=1, ls=\"-\", c=\"black\")\n",
    "ax3.plot(ind+barWidth/2., correlationEgoGlobal2, lw=1, ls=\":\", c=\"black\")\n",
    "\n",
    "ax3.fill_between(ind+barWidth/2., \n",
    "                 correlationEgoGlobal2, correlationXEgoGlobal2, \n",
    "                 where=correlationXEgoGlobal2 >= correlationEgoGlobal2, \n",
    "                 facecolor='green', alpha=0.2)\n",
    "\n",
    "ax3.fill_between(ind+barWidth/2., \n",
    "                 correlationXEgoGlobal2, correlationEgoGlobal2, \n",
    "                 where=correlationXEgoGlobal2 < correlationEgoGlobal2, \n",
    "                 facecolor='red', alpha=0.2)    \n",
    "\n",
    "ax3.text(1, 0.1, r'$\\omega=3600,\\,\\delta=3600$', fontdict=font, fontsize=18)\n",
    "\n",
    "#plt.savefig('cor_spearman_time-step-3600.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
